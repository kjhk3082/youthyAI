"""
YOUTHY AI RAG ì„œë¹„ìŠ¤ (LangChain + OpenAI)

LangChainì„ í™œìš©í•œ ê³ ê¸‰ RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ChatGPT APIì™€ ì—°ë™í•˜ì—¬ ìµœê³  í’ˆì§ˆì˜ ì •ì±… ìƒë‹´ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
1. LangChain RAG ì²´ì¸ êµ¬ì„±
2. OpenAI ChatGPT API ì—°ë™
3. ì •ì±… ë¬¸ì„œ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
4. ì¶œì²˜ ì¶”ì  ë° ì¸ìš©
5. ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬
"""

import os
import logging
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
import json

# LangChain imports
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# OpenAI
from langchain_openai import ChatOpenAI
from openai import AsyncOpenAI

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
from app.core.database import get_db_connection
import asyncpg

logger = logging.getLogger(__name__)

class YouthyRAGService:
    """
    YOUTHY AI RAG ì„œë¹„ìŠ¤
    
    LangChainì„ ì‚¬ìš©í•œ ì²´ê³„ì ì¸ RAG êµ¬í˜„ìœ¼ë¡œ
    ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ì±… ìƒë‹´ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_api_key = os.getenv('OPENAI_API_KEY', '')
        self.openai_client = None
        
        if self.openai_api_key:
            self.openai_client = AsyncOpenAI(api_key=self.openai_api_key)
            logger.info("âœ… OpenAI API ì—°ê²° ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í…œí”Œë¦¿ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._setup_embeddings()
        self._setup_memory()
        self._setup_prompts()
        
        # ê²€ìƒ‰ ê´€ë ¨
        self.retriever = None
        self.qa_chain = None
        
        # ë¡œì»¬ ìºì‹œ ì„¤ì •
        self.use_local_cache = True
        self.cache_expiry_hours = 24  # 24ì‹œê°„ í›„ ìºì‹œ ë§Œë£Œ

    def _setup_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì„¤ì •"""
        try:
            # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸
            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-m3",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.embeddings = None

    def _setup_memory(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì„¤ì •"""
        self.memory = ConversationBufferWindowMemory(
            k=5,  # ìµœê·¼ 5í„´ ëŒ€í™” ê¸°ì–µ
            memory_key="chat_history",
            return_messages=True
        )

    def _setup_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""
        
        # ì •ì±… ìƒë‹´ ì „ìš© í”„ë¡¬í”„íŠ¸
        self.policy_prompt = PromptTemplate(
            input_variables=["context", "question", "chat_history"],
            template="""
ë‹¹ì‹ ì€ YOUTHY AIì…ë‹ˆë‹¤. ìœ ì”¨ ì²­ë…„ì •ì±… ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ë‹¤ìŒê³¼ ê°™ì´ í–‰ë™í•´ì£¼ì„¸ìš”:

**ì—­í• :**
- ì²­ë…„(ë§Œ 19~39ì„¸)ì„ ìœ„í•œ ì •ì±… ì •ë³´ ì „ë¬¸ê°€
- ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ëŒ€í™”
- ì •í™•í•˜ê³  ê²€ì¦ëœ ì •ë³´ë§Œ ì œê³µ

**ì—„ê²©í•œ ë‹µë³€ ì›ì¹™:**
1. **ì˜¤ì§ ì œê³µëœ ì •ì±… ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€** - ì¶”ì¸¡ì´ë‚˜ ê°€ìƒì˜ ì •ë³´ ì ˆëŒ€ ê¸ˆì§€
2. **ì „í™”ë²ˆí˜¸**: ì •í™•í•œ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ "í•´ë‹¹ ê¸°ê´€ì— ë¬¸ì˜í•˜ì„¸ìš”" ë˜ëŠ” "ê³µì‹ í™ˆí˜ì´ì§€ì—ì„œ í™•ì¸í•˜ì„¸ìš”"
3. **ì›¹ì‚¬ì´íŠ¸ URL**: ì •í™•í•œ URLì´ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ í‘œí˜„ ì‚¬ìš© (ì˜ˆ: "í•´ë‹¹ ê¸°ê´€ì˜ ê³µì‹ í™ˆí˜ì´ì§€")
4. **ì‚¬ì—… ê¸°ê°„ êµ¬ë¶„ - í•„ìˆ˜ í¬ë§·**: 
   - **ì‚¬ì—… ìš´ì˜ê¸°ê°„**: YYYYë…„ MMì›” DDì¼ ~ YYYYë…„ MMì›” DDì¼ (ì •ì±…ì´ ì‹œí–‰ë˜ëŠ” ì „ì²´ ê¸°ê°„)
   - **ì‚¬ì—… ì‹ ì²­ê¸°ê°„**: YYYYë…„ MMì›” DDì¼ ~ YYYYë…„ MMì›” DDì¼ (ì‹¤ì œ ì‹ ì²­í•  ìˆ˜ ìˆëŠ” ê¸°ê°„)
   - ë°˜ë“œì‹œ ìœ„ í˜•ì‹ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì œê³µí•˜ê³ , ê° ê¸°ê°„ì´ ë‹¤ë¥´ë©´ ëª…í™•íˆ êµ¬ë¶„ í‘œì‹œ
5. **ë¶ˆí™•ì‹¤í•œ ì •ë³´**: "ì •í™•í•œ ì •ë³´ëŠ” í•´ë‹¹ ê¸°ê´€ì— ë¬¸ì˜í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”"
6. **ì¶œì²˜ í‘œì‹œ**: [1], [2] í˜•ì‹ìœ¼ë¡œ ë°˜ë“œì‹œ í‘œì‹œ
7. **ê°€ìƒì˜ ì˜ˆì‹œë‚˜ ê°€ì • ê¸ˆì§€**: "ì˜ˆë¥¼ ë“¤ì–´", "ì¼ë°˜ì ìœ¼ë¡œ" ê°™ì€ í‘œí˜„ìœ¼ë¡œ ì¶”ì¸¡ ì •ë³´ ì œê³µ ê¸ˆì§€

**ì •ì±… ì •ë³´:**
{context}

**ì´ì „ ëŒ€í™”:**
{chat_history}

**ì‚¬ìš©ì ì§ˆë¬¸:** {question}

**ë‹µë³€ í˜•ì‹ (í•„ìˆ˜):**
- **ì‚¬ì—… ìš´ì˜ê¸°ê°„**: YYYYë…„ MMì›” DDì¼ ~ YYYYë…„ MMì›” DDì¼
- **ì‚¬ì—… ì‹ ì²­ê¸°ê°„**: YYYYë…„ MMì›” DDì¼ ~ YYYYë…„ MMì›” DDì¼
- ê²€ì¦ëœ ì •ë³´ë§Œ ì œê³µ, ì¶œì²˜ [1], [2] í¬í•¨

**ë‹µë³€:**
"""
        )

    async def setup_retriever(self, db_connection):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì„¤ì •
        
        PostgreSQLì—ì„œ ì •ì±… ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ 
        BM25 + ë²¡í„° ê²€ìƒ‰ì„ ê²°í•©í•œ ì•™ìƒë¸” ê²€ìƒ‰ê¸°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        """
        try:
            logger.info("ğŸ” RAG ê²€ìƒ‰ê¸° ì„¤ì • ì¤‘...")
            
            # ì •ì±… ë¬¸ì„œë“¤ì„ LangChain Document í˜•íƒœë¡œ ë³€í™˜
            documents = await self._load_policy_documents(db_connection)
            
            if not documents:
                logger.warning("âš ï¸ ê²€ìƒ‰í•  ì •ì±… ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 1. BM25 ê²€ìƒ‰ê¸° (í‚¤ì›Œë“œ ê¸°ë°˜)
            bm25_retriever = BM25Retriever.from_documents(documents)
            bm25_retriever.k = 10
            
            # 2. ë²¡í„° ê²€ìƒ‰ê¸° (ì˜ë¯¸ ê¸°ë°˜) - PostgreSQL pgvector ì‚¬ìš©
            if self.embeddings:
                # PostgreSQL ì—°ê²° ì •ë³´
                db_config = {
                    'host': os.getenv('DB_HOST', 'localhost'),
                    'port': os.getenv('DB_PORT', '5432'),
                    'database': os.getenv('DB_NAME', 'youthy_ai'),
                    'user': os.getenv('DB_USER', 'postgres'),
                    'password': os.getenv('DB_PASSWORD', 'password')
                }
                
                connection_string = f"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}"
                
                vector_store = PGVector(
                    connection_string=connection_string,
                    embedding_function=self.embeddings,
                    collection_name="policy_embeddings"
                )
                
                vector_retriever = vector_store.as_retriever(search_kwargs={"k": 10})
                
                # 3. ì•™ìƒë¸” ê²€ìƒ‰ê¸° (BM25 + ë²¡í„° ê²°í•©)
                self.retriever = EnsembleRetriever(
                    retrievers=[bm25_retriever, vector_retriever],
                    weights=[0.4, 0.6]  # ë²¡í„° ê²€ìƒ‰ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
                )
            else:
                # ì„ë² ë”© ì‹¤íŒ¨ì‹œ BM25ë§Œ ì‚¬ìš©
                self.retriever = bm25_retriever
            
            logger.info("âœ… RAG ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ RAG ê²€ìƒ‰ê¸° ì„¤ì • ì‹¤íŒ¨: {e}")

    async def _load_policy_documents(self, db_connection) -> List[Document]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì •ì±… ë¬¸ì„œë“¤ì„ LangChain Documentë¡œ ë³€í™˜"""
        try:
            # í™œì„± ì •ì±…ë“¤ë§Œ ì¡°íšŒ
            policies_query = """
                SELECT p.*, 
                       string_agg(pc.chunk_text, ' ') as full_content
                FROM policies p
                LEFT JOIN policy_chunks pc ON p.id = pc.policy_id
                WHERE p.status IN ('open', 'upcoming')
                GROUP BY p.id
                ORDER BY p.updated_at DESC
                LIMIT 1000
            """
            
            rows = await db_connection.fetch(policies_query)
            documents = []
            
            for row in rows:
                # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                metadata = {
                    'policy_id': row['id'],
                    'title': row['title'],
                    'agency': row['issuing_agency'],
                    'region': row['region'],
                    'category': row['category'],
                    'source_url': row['source_url'],
                    'status': row['status'],
                    'valid_from': str(row['valid_from']) if row['valid_from'] else None,
                    'valid_to': str(row['valid_to']) if row['valid_to'] else None
                }
                
                # ë¬¸ì„œ ë‚´ìš© êµ¬ì„±
                content = f"""
ì œëª©: {row['title']}
ê¸°ê´€: {row['issuing_agency']}
ì§€ì—­: {row['region']}
ì¹´í…Œê³ ë¦¬: {', '.join(row['category'] or [])}
ìš”ì•½: {row['summary'] or ''}
ìƒì„¸ë‚´ìš©: {row['full_content'] or ''}
ì¶œì²˜: {row['source_url']}
"""
                
                doc = Document(
                    page_content=content,
                    metadata=metadata
                )
                documents.append(doc)
            
            logger.info(f"ğŸ“š {len(documents)}ê°œ ì •ì±… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ ì •ì±… ë¬¸ì„œ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return []
    
    async def search_local_policies(
        self, 
        query: str, 
        user_context: Optional[Dict] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì •ì±… ê²€ìƒ‰
        
        í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¡°í•©í•˜ì—¬
        ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ì±…ë“¤ì„ ë¹ ë¥´ê²Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        try:
            from app.core.database import _connection_pool
            
            if not _connection_pool:
                logger.warning("âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ì´ ì—†ìŠµë‹ˆë‹¤. ì™¸ë¶€ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return []
            
            async with _connection_pool.acquire() as conn:
                # ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
                base_query = """
                    SELECT 
                        id, title, summary, body_html, issuing_agency, 
                        program_type, region, category, eligibility, 
                        benefit, apply_method, contact, source_url, 
                        source_name, valid_from, valid_to, status,
                        updated_at
                    FROM policies 
                    WHERE status = 'open'
                    AND (valid_to IS NULL OR valid_to >= CURRENT_DATE)
                """
                
                conditions = []
                params = []
                param_count = 0
                
                # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¡°ê±´ ì¶”ê°€
                if query.strip():
                    param_count += 1
                    conditions.append(f"""
                        (title ILIKE ${param_count} 
                         OR summary ILIKE ${param_count}
                         OR body_html ILIKE ${param_count})
                    """)
                    params.append(f"%{query}%")
                
                # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°ë§
                if user_context:
                    # ë‚˜ì´ ì¡°ê±´
                    if 'age' in user_context:
                        age = user_context['age']
                        conditions.append(f"""
                            (eligibility->>'age' IS NULL 
                             OR (eligibility->'age'->>'min')::int <= {age}
                             AND (eligibility->'age'->>'max')::int >= {age})
                        """)
                    
                    # ì§€ì—­ ì¡°ê±´
                    if 'region' in user_context:
                        param_count += 1
                        conditions.append(f"""
                            (region = 'ì„œìš¸ì‹œ ì „ì²´' 
                             OR region = ${param_count})
                        """)
                        params.append(user_context['region'])
                    
                    # í•™ìƒ ì—¬ë¶€
                    if user_context.get('student'):
                        conditions.append("""
                            (eligibility->>'student' IS NULL 
                             OR eligibility->>'student' = 'true')
                        """)
                
                # ì¡°ê±´ë“¤ì„ ì¿¼ë¦¬ì— ì¶”ê°€
                if conditions:
                    base_query += " AND " + " AND ".join(conditions)
                
                # ì •ë ¬ ë° ì œí•œ
                base_query += f"""
                    ORDER BY 
                        CASE WHEN title ILIKE $1 THEN 1 ELSE 2 END,
                        updated_at DESC
                    LIMIT {limit}
                """
                
                # ì¿¼ë¦¬ ì‹¤í–‰
                rows = await conn.fetch(base_query, *params)
                
                # ê²°ê³¼ ë³€í™˜
                policies = []
                for row in rows:
                    policy = {
                        'id': row['id'],
                        'title': row['title'],
                        'summary': row['summary'],
                        'content': row['body_html'],
                        'agency': row['issuing_agency'],
                        'program_type': row['program_type'],
                        'region': row['region'],
                        'category': row['category'],
                        'eligibility': row['eligibility'],
                        'benefit': row['benefit'],
                        'apply_method': row['apply_method'],
                        'contact': row['contact'],
                        'source_url': row['source_url'],
                        'source_name': row['source_name'],
                        'valid_from': str(row['valid_from']) if row['valid_from'] else None,
                        'valid_to': str(row['valid_to']) if row['valid_to'] else None,
                        'updated_at': row['updated_at'].isoformat() if row['updated_at'] else None
                    }
                    policies.append(policy)
                
                logger.info(f"ğŸ” ë¡œì»¬ DBì—ì„œ {len(policies)}ê°œ ì •ì±… ê²€ìƒ‰ ì™„ë£Œ")
                return policies
                
        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ ì •ì±… ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _convert_policies_to_docs(self, policies: List[Dict[str, Any]]) -> List[Document]:
        """
        ì •ì±… ë”•ì…”ë„ˆë¦¬ë¥¼ LangChain Document ê°ì²´ë¡œ ë³€í™˜
        """
        documents = []
        
        for policy in policies:
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                'policy_id': policy['id'],
                'title': policy['title'],
                'agency': policy['agency'],
                'region': policy['region'],
                'category': policy['category'],
                'source_url': policy['source_url'],
                'source_name': policy['source_name'],
                'status': 'open',
                'valid_from': policy['valid_from'],
                'valid_to': policy['valid_to']
            }
            
            # ë¬¸ì„œ ë‚´ìš© êµ¬ì„±
            content = f"""
ì œëª©: {policy['title']}
ê¸°ê´€: {policy['agency']}
ì§€ì—­: {policy['region']}
ì¹´í…Œê³ ë¦¬: {', '.join(policy['category'] or [])}
ìš”ì•½: {policy['summary'] or ''}
ìƒì„¸ë‚´ìš©: {policy['content'] or ''}
ì¶œì²˜: {policy['source_url']}
"""
            
            doc = Document(
                page_content=content,
                metadata=metadata
            )
            documents.append(doc)
        
        return documents

    async def chat_with_rag(
        self, 
        user_message: str, 
        user_context: Optional[Dict] = None,
        conversation_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        RAG ê¸°ë°˜ ì±„íŒ… ì‘ë‹µ ìƒì„±
        
        LangChainì˜ RetrievalQA ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬
        ì •ì±… ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ChatGPTë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            if not self.openai_client:
                # OpenAI APIê°€ ì—†ìœ¼ë©´ í…œí”Œë¦¿ ê¸°ë°˜ ì‘ë‹µ
                return await self._generate_template_response(user_message, user_context)
            
            # 1. ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìš°ì„  ê²€ìƒ‰ (ë¹ ë¥¸ ì‘ë‹µ)
            local_policies = []
            if self.use_local_cache:
                local_policies = await self.search_local_policies(
                    user_message, user_context, limit=5
                )
                logger.info(f"ğŸ’¾ ë¡œì»¬ ìºì‹œì—ì„œ {len(local_policies)}ê°œ ì •ì±… ë°œê²¬")
            
            # 2. ë¡œì»¬ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ì™¸ë¶€ ê²€ìƒ‰ ì¶”ê°€
            relevant_docs = []
            if len(local_policies) >= 3:
                # ë¡œì»¬ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë©´ ë¡œì»¬ ë°ì´í„°ë§Œ ì‚¬ìš©
                relevant_docs = self._convert_policies_to_docs(local_policies)
                logger.info("âš¡ ë¡œì»¬ ìºì‹œ ë°ì´í„°ë¡œ ë¹ ë¥¸ ì‘ë‹µ ìƒì„±")
            else:
                # ë¡œì»¬ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ì™¸ë¶€ ê²€ìƒ‰ ë³‘í–‰
                if self.retriever:
                    external_docs = await self._retrieve_relevant_policies(user_message, user_context)
                    relevant_docs = self._convert_policies_to_docs(local_policies) + external_docs
                    logger.info(f"ğŸ” ë¡œì»¬({len(local_policies)}) + ì™¸ë¶€({len(external_docs)}) ê²€ìƒ‰ ê²°í•©")
                else:
                    relevant_docs = self._convert_policies_to_docs(local_policies)
            
            # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context_from_docs(relevant_docs)
            
            # 4. ChatGPT APIë¡œ ì‘ë‹µ ìƒì„± (RAG Generation)
            response = await self._generate_with_openai(user_message, context, user_context)
            
            # 5. ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
            references = self._extract_references_from_docs(relevant_docs)
            
            return {
                'answer': response,
                'references': references,
                'confidence_score': 0.9 if relevant_docs else 0.3,
                'context_used': len(relevant_docs),
                'model_used': 'gpt-3.5-turbo',
                'cache_used': len(local_policies) > 0
            }
            
        except Exception as e:
            logger.error(f"âŒ RAG ì±„íŒ… ì˜¤ë¥˜: {e}")
            return await self._generate_fallback_response(user_message)

    async def _retrieve_relevant_policies(
        self, 
        query: str, 
        user_context: Optional[Dict] = None
    ) -> List[Document]:
        """ê´€ë ¨ ì •ì±… ë¬¸ì„œ ê²€ìƒ‰ (ì˜¨í†µì²­ë…„ API + ê¸°ì¡´ ë°ì´í„° í†µí•©)"""
        try:
            all_docs = []
            
            # 1. ì˜¨í†µì²­ë…„ APIì—ì„œ ê²€ìƒ‰
            try:
                from app.services.youthcenter_api import youthcenter_client
                
                # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì˜¨í†µì²­ë…„ ì •ì±… ê²€ìƒ‰
                region = user_context.get('region') if user_context else None
                age = user_context.get('age') if user_context else None
                
                youthcenter_policies = await youthcenter_client.search_policies_by_region_and_age(
                    region=region, age=age, max_results=10
                )
                
                # ì˜¨í†µì²­ë…„ ì •ì±…ì„ Document í˜•íƒœë¡œ ë³€í™˜
                for policy in youthcenter_policies:
                    doc_content = youthcenter_client.format_policy_for_llm(policy)
                    
                    # ì •ì±…ì„ 8ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ìë™ ë¶„ë¥˜
                    auto_category = youthcenter_client.classify_policy_category(policy)
                    
                    doc = Document(
                        page_content=doc_content,
                        metadata={
                            'title': policy.get('polyBizSjnm', 'N/A'),
                            'agency': policy.get('cnsgNmor', 'N/A'),
                            'region': 'ì „êµ­' if not region else region,
                            'category': [auto_category],  # 8ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜
                            'original_category': policy.get('polyRlmCd', 'ê¸°íƒ€'),
                            'status': 'ìš´ì˜ì¤‘',
                            'source_url': policy.get('rfcSiteUrla1', 'N/A'),
                            'source': 'ì˜¨í†µì²­ë…„',
                            'policy_id': policy.get('bizId', ''),
                            'support_target': policy.get('sporTarget', 'N/A'),
                            'support_content': policy.get('sporCn', 'N/A'),
                            'application_period': policy.get('rqutPrdCn', 'N/A'),
                            'application_method': policy.get('rqutProcCn', 'N/A'),
                            'last_updated': datetime.now().isoformat(),
                            'matched_category': policy.get('matched_category', auto_category)
                        }
                    )
                    all_docs.append(doc)
                
                logger.info(f"âœ… ì˜¨í†µì²­ë…„ API: {len(youthcenter_policies)}ê°œ ì •ì±… ê²€ìƒ‰")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ì˜¨í†µì²­ë…„ API ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # 2. ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ë„ ë³‘í–‰ (ìˆë‹¤ë©´)
            if self.retriever:
                try:
                    # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ì¿¼ë¦¬ì— ë°˜ì˜
                    enhanced_query = self._enhance_query_with_context(query, user_context)
                    
                    # LangChain ê²€ìƒ‰ ì‹¤í–‰
                    local_docs = await self.retriever.aget_relevant_documents(enhanced_query)
                    
                    # ì‚¬ìš©ì ì¡°ê±´ì— ë§ëŠ” ì •ì±…ë§Œ í•„í„°ë§
                    filtered_local_docs = self._filter_docs_by_user_context(local_docs, user_context)
                    
                    all_docs.extend(filtered_local_docs)
                    logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„°: {len(filtered_local_docs)}ê°œ ì •ì±… ê²€ìƒ‰")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # 3. ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ë„ ìˆœ ì •ë ¬
            unique_docs = self._deduplicate_and_rank_docs(all_docs, query)
            
            logger.info(f"ğŸ” í†µí•© ê²€ìƒ‰ ì™„ë£Œ: {len(unique_docs)}ê°œ ê´€ë ¨ ì •ì±… ë°œê²¬")
            return unique_docs[:8]  # ìµœëŒ€ 8ê°œ (ì˜¨í†µì²­ë…„ + ê¸°ì¡´ ë°ì´í„°)
            
        except Exception as e:
            logger.error(f"âŒ ì •ì±… ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _enhance_query_with_context(self, query: str, user_context: Optional[Dict]) -> str:
        """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ í–¥ìƒ"""
        enhanced_parts = [query]
        
        if user_context:
            if user_context.get('region'):
                enhanced_parts.append(f"ì§€ì—­: {user_context['region']}")
            
            if user_context.get('age'):
                enhanced_parts.append(f"ë‚˜ì´: {user_context['age']}ì„¸")
            
            if user_context.get('student'):
                enhanced_parts.append("ëŒ€í•™ìƒ")
        
        return ' '.join(enhanced_parts)

    def _filter_docs_by_user_context(self, docs: List[Document], user_context: Optional[Dict]) -> List[Document]:
        """ì‚¬ìš©ì ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œë§Œ í•„í„°ë§"""
        if not user_context:
            return docs
        
        filtered = []
        
        for doc in docs:
            metadata = doc.metadata
            
            # ì§€ì—­ í•„í„°ë§
            if user_context.get('region'):
                doc_region = metadata.get('region', '')
                if doc_region != 'ì„œìš¸ì‹œ ì „ì²´' and doc_region != user_context['region']:
                    continue
            
            # ë‚˜ì´ í•„í„°ë§ (eligibility ì •ë³´ê°€ ìˆë‹¤ë©´)
            if user_context.get('age'):
                # ì‹¤ì œë¡œëŠ” eligibility JSONì„ íŒŒì‹±í•´ì„œ ë‚˜ì´ ì¡°ê±´ í™•ì¸
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í†µê³¼
                pass
            
            filtered.append(doc)
        
        return filtered

    def _deduplicate_and_rank_docs(self, docs: List[Document], query: str) -> List[Document]:
        """ë¬¸ì„œ ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ë„ ìˆœ ì •ë ¬"""
        if not docs:
            return []
        
        # ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±°
        seen_titles = set()
        unique_docs = []
        
        for doc in docs:
            title = doc.metadata.get('title', '').strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_docs.append(doc)
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê´€ë ¨ë„ ê³„ì‚°
        query_keywords = set(query.lower().split())
        
        def calculate_relevance(doc: Document) -> float:
            content = doc.page_content.lower()
            title = doc.metadata.get('title', '').lower()
            
            # ì œëª© ë§¤ì¹­ ì ìˆ˜ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
            title_score = sum(1 for keyword in query_keywords if keyword in title) * 3
            
            # ë‚´ìš© ë§¤ì¹­ ì ìˆ˜
            content_score = sum(1 for keyword in query_keywords if keyword in content)
            
            # ì˜¨í†µì²­ë…„ ë°ì´í„° ìš°ì„  ìˆœìœ„ (ë” ìƒì„¸í•œ ì •ë³´)
            source_bonus = 2 if doc.metadata.get('source') == 'ì˜¨í†µì²­ë…„' else 0
            
            return title_score + content_score + source_bonus
        
        # ê´€ë ¨ë„ ìˆœ ì •ë ¬
        unique_docs.sort(key=calculate_relevance, reverse=True)
        
        return unique_docs

    def _build_context_from_docs(self, docs: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not docs:
            return "ê´€ë ¨ ì •ì±… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        
        for i, doc in enumerate(docs, 1):
            metadata = doc.metadata
            
            context_part = f"""
[ì •ì±… {i}]
ì œëª©: {metadata.get('title', 'ì œëª© ì—†ìŒ')}
ê¸°ê´€: {metadata.get('agency', 'ê¸°ê´€ ì—†ìŒ')}
ì§€ì—­: {metadata.get('region', 'ì§€ì—­ ì—†ìŒ')}
ì¹´í…Œê³ ë¦¬: {', '.join(metadata.get('category', []))}
ìƒíƒœ: {metadata.get('status', 'ìƒíƒœ ì—†ìŒ')}
ì¶œì²˜: {metadata.get('source_url', 'URL ì—†ìŒ')}

ë‚´ìš©:
{doc.page_content[:800]}...

---
"""
            context_parts.append(context_part)
        
        return '\n'.join(context_parts)

    def _extract_references_from_docs(self, docs: List[Document]) -> List[Dict]:
        """ë¬¸ì„œë“¤ì—ì„œ ì°¸ì¡° ì •ë³´ ì¶”ì¶œ"""
        references = []
        
        for i, doc in enumerate(docs, 1):
            metadata = doc.metadata
            
            # ìŠ¤ë‹ˆí« ìƒì„± (ì²« 200ì)
            snippet = doc.page_content[:200].replace('\n', ' ').strip()
            if len(doc.page_content) > 200:
                snippet += '...'
            
            references.append({
                'id': f'ref_{i}',
                'title': metadata.get('title', 'ì œëª© ì—†ìŒ'),
                'url': metadata.get('source_url', '#'),
                'snippet': snippet,
                'source': metadata.get('agency', 'ê¸°ê´€ ì—†ìŒ'),
                'relevance_score': 0.8,  # LangChainì—ì„œ ì‹¤ì œ ì ìˆ˜ ë°›ì•„ì˜¬ ìˆ˜ ìˆìŒ
                'last_updated': metadata.get('updated_at', datetime.now().isoformat())
            })
        
        return references

    async def _generate_with_openai(
        self, 
        user_message: str, 
        context: str, 
        user_context: Optional[Dict] = None
    ) -> str:
        """OpenAI ChatGPT APIë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            # ì‚¬ìš©ì ì •ë³´ ì¶”ê°€
            user_info = ""
            if user_context:
                info_parts = []
                if user_context.get('age'):
                    info_parts.append(f"ë‚˜ì´: {user_context['age']}ì„¸")
                if user_context.get('region'):
                    info_parts.append(f"ê±°ì£¼ì§€: {user_context['region']}")
                if user_context.get('student'):
                    info_parts.append("ëŒ€í•™ìƒ")
                
                if info_parts:
                    user_info = f"ì‚¬ìš©ì ì •ë³´: {', '.join(info_parts)}\n\n"
            
            # ChatGPT API í˜¸ì¶œ
            messages = [
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ YOUTHY AI, ìœ ì”¨ ì²­ë…„ì •ì±… ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ë‹µë³€ ê·œì¹™:**
1. ì œê³µëœ ì •ì±… ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€
2. ëª¨ë“  ì •ì±…ì— ì¶œì²˜ ë²ˆí˜¸ [1], [2], [3] í‘œì‹œ
3. ì‚¬ìš©ì ìƒí™©ì— ë§ëŠ” êµ¬ì²´ì  ì¡°ì–¸ í¬í•¨
4. ì‹ ì²­ ë°©ë²•ê³¼ ì—°ë½ì²˜ë¥¼ ëª…í™•íˆ ì•ˆë‚´
5. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ ìœ ì§€

**ì‘ë‹µ êµ¬ì¡°:**
1. ì¸ì‚¬ ë° ìƒí™© íŒŒì•…
2. ê´€ë ¨ ì •ì±…ë“¤ ì†Œê°œ (ë²ˆí˜¸ì™€ ì¶œì²˜ í¬í•¨)
3. ê°œì¸í™”ëœ ì¡°ì–¸
4. ì¶”ê°€ ë¬¸ì˜ ì•ˆë‚´"""
                },
                {
                    "role": "user",
                    "content": f"{user_info}ì •ì±… ì •ë³´:\n{context}\n\nì§ˆë¬¸: {user_message}"
                }
            ]
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=1500,
                temperature=0.7,
                stream=False
            )
            
            answer = response.choices[0].message.content
            return answer
            
        except Exception as e:
            logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return "OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

    async def generate_streaming_response(
        self, 
        user_message: str, 
        context: str, 
        user_context: Optional[Dict] = None
    ) -> AsyncGenerator[str, None]:
        """OpenAI ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        try:
            if not self.openai_client:
                # OpenAIê°€ ì—†ìœ¼ë©´ í…œí”Œë¦¿ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°
                response = await self._generate_template_response(user_message, user_context)
                sentences = response['answer'].split('\n')
                for sentence in sentences:
                    if sentence.strip():
                        yield sentence + '\n'
                        await asyncio.sleep(0.1)
                return
            
            # ì‚¬ìš©ì ì •ë³´ êµ¬ì„±
            user_info = ""
            if user_context:
                info_parts = []
                if user_context.get('age'):
                    info_parts.append(f"ë‚˜ì´: {user_context['age']}ì„¸")
                if user_context.get('region'):
                    info_parts.append(f"ê±°ì£¼ì§€: {user_context['region']}")
                if user_context.get('student'):
                    info_parts.append("ëŒ€í•™ìƒ")
                
                if info_parts:
                    user_info = f"ì‚¬ìš©ì ì •ë³´: {', '.join(info_parts)}\n\n"
            
            # OpenAI ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
            messages = [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ YOUTHY AI, ìœ ì”¨ ì²­ë…„ì •ì±… ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì¹œê·¼í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": f"{user_info}ì •ì±… ì •ë³´:\n{context}\n\nì§ˆë¬¸: {user_message}"
                }
            ]
            
            stream = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=1500,
                temperature=0.7,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"âŒ OpenAI ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield "ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def _generate_template_response(self, user_message: str, user_context: Optional[Dict]) -> Dict[str, Any]:
        """í…œí”Œë¦¿ ê¸°ë°˜ ëŒ€ì²´ ì‘ë‹µ (OpenAI ì—†ì„ ë•Œ)"""
        return {
            'answer': f"""
ì•ˆë…•í•˜ì„¸ìš”! YOUTHY AIì…ë‹ˆë‹¤. ğŸ˜Š

í˜„ì¬ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ í…œí”Œë¦¿ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.

**ì§ˆë¬¸**: {user_message}

**ì„ì‹œ ë‹µë³€**: 
ì£„ì†¡í•©ë‹ˆë‹¤. ì •í™•í•œ AI ë‹µë³€ì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:

1. **OpenAI API í‚¤ ì„¤ì •** (ê¶Œì¥)
   ```bash
   export OPENAI_API_KEY=sk-your-key-here
   ```

2. **Ollama ë¡œì»¬ ì‹¤í–‰**
   ```bash
   brew install ollama
   ollama pull llama2:7b-chat
   ollama serve
   ```

3. **Hugging Face í† í° ì„¤ì •**
   ```bash
   export HF_API_TOKEN=your-token-here
   ```

ì„¤ì • í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ì •í™•í•œ ì •ì±… ì •ë³´ë¥¼ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ™
""",
            'references': [],
            'confidence_score': 0.1
        }

    async def _generate_fallback_response(self, user_message: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ì‹œ ëŒ€ì²´ ì‘ë‹µ"""
        return {
            'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ™",
            'references': [],
            'confidence_score': 0.0,
            'model_used': 'fallback'
        }

    def add_conversation_to_memory(self, user_message: str, ai_response: str):
        """ëŒ€í™”ë¥¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€"""
        try:
            self.memory.chat_memory.add_user_message(user_message)
            self.memory.chat_memory.add_ai_message(ai_response)
        except Exception as e:
            logger.warning(f"âš ï¸ ëŒ€í™” ë©”ëª¨ë¦¬ ì¶”ê°€ ì‹¤íŒ¨: {e}")

    def get_conversation_history(self) -> List[Dict]:
        """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
        try:
            messages = self.memory.chat_memory.messages
            history = []
            
            for msg in messages:
                history.append({
                    'role': 'user' if msg.type == 'human' else 'assistant',
                    'content': msg.content,
                    'timestamp': datetime.now().isoformat()
                })
            
            return history
            
        except Exception as e:
            logger.error(f"âŒ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

# ========================================
# RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# ========================================

# ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_rag_service: Optional[YouthyRAGService] = None

async def get_rag_service() -> YouthyRAGService:
    """RAG ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_service
    
    if _rag_service is None:
        _rag_service = YouthyRAGService()
        logger.info("âœ… RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    return _rag_service

async def initialize_rag_system(db_connection):
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    try:
        rag_service = await get_rag_service()
        await rag_service.setup_retriever(db_connection)
        logger.info("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ========================================
# ì„¤ì • ê°€ì´ë“œ
# ========================================

def print_llm_setup_guide():
    """LLM ì„¤ì • ê°€ì´ë“œ ì¶œë ¥"""
    print("""
ğŸ¤– YOUTHY AI LLM ì„¤ì • ê°€ì´ë“œ

**ğŸ¯ ì¶”ì²œ ìˆœì„œ:**

1. ğŸ¥‡ OpenAI ChatGPT (ìµœê³  í’ˆì§ˆ, ìœ ë£Œ)
   ```bash
   export OPENAI_API_KEY=sk-your-key-here
   ```
   - ë¹„ìš©: $0.002/1K í† í° (ë§¤ìš° ì €ë ´)
   - í’ˆì§ˆ: â­â­â­â­â­
   - ì†ë„: â­â­â­â­â­

2. ğŸ¥ˆ Ollama (ë¬´ë£Œ, ë¡œì»¬, ê³ í’ˆì§ˆ)
   ```bash
   # ì„¤ì¹˜
   brew install ollama  # macOS
   # ë˜ëŠ” https://ollama.ai
   
   # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
   ollama pull llama2:7b-chat
   ollama serve
   
   export LLM_TYPE=ollama
   ```
   - ë¹„ìš©: ë¬´ë£Œ
   - í’ˆì§ˆ: â­â­â­â­
   - ì†ë„: â­â­â­

3. ğŸ¥‰ Hugging Face (ë¬´ë£Œ API)
   ```bash
   # í† í° ë°œê¸‰: https://huggingface.co/settings/tokens
   export HF_API_TOKEN=your-token-here
   export LLM_TYPE=huggingface
   ```
   - ë¹„ìš©: ë¬´ë£Œ (ì œí•œ ìˆìŒ)
   - í’ˆì§ˆ: â­â­â­
   - ì†ë„: â­â­

4. ğŸš€ í…œí”Œë¦¿ ëª¨ë“œ (ì¦‰ì‹œ ì‹¤í–‰)
   - ì„¤ì • ë¶ˆí•„ìš”
   - êµ¬ì¡°í™”ëœ ì‘ë‹µ
   - í’ˆì§ˆ: â­â­
   - ì†ë„: â­â­â­â­â­

**ğŸ’¡ ê¶Œì¥ì‚¬í•­:**
- MVP/ë°ëª¨: í…œí”Œë¦¿ ëª¨ë“œë¡œ ì‹œì‘
- ê°œë°œ/í…ŒìŠ¤íŠ¸: Ollama ì‚¬ìš©
- ìš´ì˜: OpenAI ChatGPT ì‚¬ìš©
""")

if __name__ == "__main__":
    print_llm_setup_guide()
