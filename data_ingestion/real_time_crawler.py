"""
ì‹¤ì‹œê°„ ì„œìš¸ ì²­ë…„ì •ì±… í¬ë¡¤ëŸ¬

2025ë…„ 9ì›” ê¸°ì¤€ ì‹¤ì œ ìš´ì˜ë˜ëŠ” ì„œìš¸ì‹œ ì²­ë…„ì •ì±… ì›¹ì‚¬ì´íŠ¸ë“¤ì—ì„œ
ìµœì‹  ì •ì±… ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

íŒŒì‹± ëŒ€ìƒ ì‚¬ì´íŠ¸:
1. ì„œìš¸ì‹œ ì²­ë…„í¬í„¸ (youth.seoul.go.kr)
2. ì„œìš¸ì‹œ ê³µì‹ í™ˆí˜ì´ì§€ ì •ì±… ì„¹ì…˜
3. ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API
4. ê° ìì¹˜êµ¬ ì²­ë…„ì •ì±… í˜ì´ì§€
5. ì„œìš¸ì‹œ ê³ ì‹œê³µê³ 

í•µì‹¬ ê¸°ëŠ¥:
- ì‹¤ì‹œê°„ ì›¹ í¬ë¡¤ë§
- ë³€ê²½ì‚¬í•­ ê°ì§€
- ì •ì±… ë¶„ë¥˜ ë° ì •ê·œí™”
- ì¶œì²˜ URL ì¶”ì 
- ì˜¤ë¥˜ ë³µêµ¬ ë° ì¬ì‹œë„
"""

import asyncio
import aiohttp
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
import hashlib
import json
import re

from bs4 import BeautifulSoup
import feedparser  # RSS íŒŒì‹±ìš©
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class PolicyData:
    """íŒŒì‹±ëœ ì •ì±… ë°ì´í„° êµ¬ì¡°"""
    title: str
    summary: str
    content: str
    agency: str
    category: List[str]
    target_age: Optional[Dict[str, int]]
    target_region: str
    benefits: str
    apply_method: str
    apply_url: str
    contact_info: str
    valid_from: Optional[datetime]
    valid_to: Optional[datetime]
    source_url: str
    source_name: str

class SeoulYouthPolicyCrawler:
    """
    ìœ ì”¨ ì²­ë…„ì •ì±… ì‹¤ì‹œê°„ í¬ë¡¤ëŸ¬
    
    ì‹¤ì œ ìš´ì˜ ì¤‘ì¸ ì›¹ì‚¬ì´íŠ¸ë“¤ì—ì„œ ìµœì‹  ì •ì±… ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    ë³€ê²½ì‚¬í•­ì„ ê°ì§€í•˜ê³  ìƒˆë¡œìš´ ì •ì±…ë§Œ ì—…ë°ì´íŠ¸í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    """
    
    def __init__(self, api_key: str = None):
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ (ë³´ì•ˆ)
        import os
        self.api_key = api_key or os.getenv('SEOUL_OPEN_DATA_API_KEY', '')
        self.session = None
        self.crawl_stats = {
            'total_pages_crawled': 0,
            'policies_found': 0,
            'new_policies': 0,
            'updated_policies': 0,
            'errors': 0
        }
        
        # í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì„¤ì • (2025ë…„ ì‹¤ì œ URLë“¤)
        self.target_sites = {
            'seoul_youth_portal': {
                'base_url': 'https://youth.seoul.go.kr',
                'policy_list_urls': [
                    '/site/main/archive/policy',  # ì •ì±… ì•„ì¹´ì´ë¸Œ
                    '/site/main/content/policy_info',  # ì •ì±… ì •ë³´
                    '/site/main/content/support_program'  # ì§€ì› í”„ë¡œê·¸ë¨
                ],
                'selectors': {
                    'policy_links': 'a[href*="/policy/"]',
                    'title': 'h1, .policy-title, .title',
                    'content': '.policy-content, .content, main',
                    'agency': '.agency, .department',
                    'period': '.period, .apply-period'
                }
            },
            'seoul_main_site': {
                'base_url': 'https://www.seoul.go.kr',
                'policy_list_urls': [
                    '/policy/youth',  # ì²­ë…„ì •ì±…
                    '/news/notice?category=youth',  # ì²­ë…„ ê´€ë ¨ ê³µê³ 
                ],
                'selectors': {
                    'policy_links': 'a[href*="/policy/"], a[href*="/notice/"]',
                    'title': 'h1, .notice-title, .policy-title',
                    'content': '.notice-content, .policy-content',
                    'date': '.date, .reg-date'
                }
            },
            'district_sites': {
                # 25ê°œ ìì¹˜êµ¬ ì²­ë…„ì •ì±… í˜ì´ì§€ (ì£¼ìš” êµ¬ë§Œ ì„ ë³„)
                'gangnam': 'https://www.gangnam.go.kr/youth',
                'songpa': 'https://www.songpa.go.kr/youth',
                'seocho': 'https://www.seocho.go.kr/youth',
                'mapo': 'https://www.mapo.go.kr/youth',
                'seongbuk': 'https://www.seongbuk.go.kr/youth'
            }
        }

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € - ì„¸ì…˜ ì‹œì‘"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'YOUTHY-AI-Bot/1.0 (Youth Policy Information Service)'
            }
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € - ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session:
            await self.session.close()

    async def crawl_all_sources(self) -> List[PolicyData]:
        """
        ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì •ì±… ì •ë³´ ìˆ˜ì§‘
        
        Returns:
            List[PolicyData]: ìˆ˜ì§‘ëœ ì •ì±… ë°ì´í„° ëª©ë¡
        """
        logger.info("ğŸ•·ï¸ ì‹¤ì‹œê°„ ì •ì±… í¬ë¡¤ë§ ì‹œì‘...")
        
        all_policies = []
        
        try:
            # 1. ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API í˜¸ì¶œ
            api_policies = await self._crawl_seoul_open_data_api()
            all_policies.extend(api_policies)
            
            # 2. ì„œìš¸ì²­ë…„í¬í„¸ í¬ë¡¤ë§
            portal_policies = await self._crawl_youth_portal()
            all_policies.extend(portal_policies)
            
            # 3. ì„œìš¸ì‹œ ê³µì‹ í™ˆí˜ì´ì§€ í¬ë¡¤ë§
            main_site_policies = await self._crawl_seoul_main_site()
            all_policies.extend(main_site_policies)
            
            # 4. ì£¼ìš” ìì¹˜êµ¬ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
            district_policies = await self._crawl_district_sites()
            all_policies.extend(district_policies)
            
            # 5. RSS í”¼ë“œ ìˆ˜ì§‘
            rss_policies = await self._crawl_rss_feeds()
            all_policies.extend(rss_policies)
            
            logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_policies)}ê°œ ì •ì±… ìˆ˜ì§‘")
            self._log_crawl_stats()
            
            return all_policies
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    async def _crawl_seoul_open_data_api(self) -> List[PolicyData]:
        """ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ APIì—ì„œ ì •ì±… ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("ğŸ“¡ ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API í˜¸ì¶œ...")
        
        policies = []
        
        # ì‹¤ì œ 2025ë…„ ìš´ì˜ ì¤‘ì¸ ì²­ë…„ì •ì±… ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë“¤
        api_endpoints = [
            {
                'name': 'ì„œìš¸ì‹œì²­ë…„ì •ì±…ì •ë³´',
                'url': f'http://openapi.seoul.go.kr:8088/{self.api_key}/json/SeoulYouthPolicyInfo/1/100/',
                'description': 'ì„œìš¸ì‹œ ì²­ë…„ì •ì±… ì¢…í•© ì •ë³´'
            },
            {
                'name': 'ì²­ë…„ì§€ì›ì‚¬ì—…í˜„í™©',
                'url': f'http://openapi.seoul.go.kr:8088/{self.api_key}/json/YouthSupportStatus/1/100/',
                'description': 'ì²­ë…„ì§€ì›ì‚¬ì—… í˜„í™©'
            },
            {
                'name': 'ìì¹˜êµ¬ë³„ì²­ë…„ì •ì±…',
                'url': f'http://openapi.seoul.go.kr:8088/{self.api_key}/json/DistrictYouthPolicy/1/100/',
                'description': '25ê°œ ìì¹˜êµ¬ë³„ ì²­ë…„ì •ì±…'
            }
        ]
        
        for endpoint in api_endpoints:
            try:
                async with self.session.get(endpoint['url']) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        # API ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ë°ì´í„° ì¶”ì¶œ
                        if 'row' in data and data['row']:
                            for item in data['row']:
                                policy = await self._parse_api_response(item, endpoint['name'])
                                if policy:
                                    policies.append(policy)
                                    
                        logger.info(f"âœ… {endpoint['name']}: {len(data.get('row', []))}ê°œ ìˆ˜ì§‘")
                        
                    else:
                        logger.warning(f"âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨ {endpoint['name']}: {response.status}")
                        
                # API í˜¸ì¶œ ì œí•œ ì¤€ìˆ˜
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜ {endpoint['name']}: {e}")
                self.crawl_stats['errors'] += 1
        
        self.crawl_stats['policies_found'] += len(policies)
        return policies

    async def _crawl_youth_portal(self) -> List[PolicyData]:
        """ì„œìš¸ì²­ë…„í¬í„¸ ì›¹ í¬ë¡¤ë§"""
        logger.info("ğŸŒ ì„œìš¸ì²­ë…„í¬í„¸ í¬ë¡¤ë§...")
        
        policies = []
        site_config = self.target_sites['seoul_youth_portal']
        
        try:
            # ì •ì±… ëª©ë¡ í˜ì´ì§€ë“¤ ìˆœíšŒ
            for list_url in site_config['policy_list_urls']:
                full_url = site_config['base_url'] + list_url
                
                try:
                    async with self.session.get(full_url) as response:
                        if response.status == 200:
                            html = await response.text()
                            soup = BeautifulSoup(html, 'html.parser')
                            
                            # ì •ì±… ë§í¬ ì¶”ì¶œ
                            policy_links = soup.select(site_config['selectors']['policy_links'])
                            
                            logger.info(f"ğŸ“„ {list_url}ì—ì„œ {len(policy_links)}ê°œ ì •ì±… ë§í¬ ë°œê²¬")
                            
                            # ê° ì •ì±… ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
                            for link in policy_links[:20]:  # MVP: í˜ì´ì§€ë‹¹ ìµœëŒ€ 20ê°œ
                                href = link.get('href')
                                if href:
                                    policy_url = urljoin(site_config['base_url'], href)
                                    policy = await self._crawl_policy_detail(
                                        policy_url, 
                                        site_config, 
                                        'ì„œìš¸ì²­ë…„í¬í„¸'
                                    )
                                    if policy:
                                        policies.append(policy)
                                    
                                # í¬ë¡¤ë§ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                                await asyncio.sleep(0.5)
                                
                        self.crawl_stats['total_pages_crawled'] += 1
                        
                except Exception as e:
                    logger.error(f"âŒ í˜ì´ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜ {full_url}: {e}")
                    
        except Exception as e:
            logger.error(f"âŒ ì²­ë…„í¬í„¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
        return policies

    async def _crawl_policy_detail(self, url: str, site_config: Dict, source_name: str) -> Optional[PolicyData]:
        """ê°œë³„ ì •ì±… ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return None
                    
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ
                title_elem = soup.select_one(site_config['selectors']['title'])
                title = title_elem.get_text().strip() if title_elem else ''
                
                if not title or len(title) < 5:
                    return None
                
                # ë‚´ìš© ì¶”ì¶œ
                content_elem = soup.select_one(site_config['selectors']['content'])
                content = content_elem.get_text().strip() if content_elem else ''
                
                # ê¸°ê´€ ì •ë³´ ì¶”ì¶œ
                agency_elem = soup.select_one(site_config['selectors'].get('agency', ''))
                agency = agency_elem.get_text().strip() if agency_elem else 'ì„œìš¸íŠ¹ë³„ì‹œ'
                
                # ì •ì±… ë¶„ì„ ë° êµ¬ì¡°í™”
                parsed_policy = self._analyze_policy_content(title, content, url, source_name, agency)
                
                return parsed_policy
                
        except Exception as e:
            logger.warning(f"âš ï¸ ì •ì±… ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
            return None

    def _analyze_policy_content(self, title: str, content: str, url: str, source: str, agency: str) -> PolicyData:
        """
        ì •ì±… ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
        
        AIë¥¼ í™œìš©í•˜ì—¬ ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ ì •í˜• ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤:
        - ëŒ€ìƒ ì—°ë ¹ ì¶”ì¶œ
        - ì§€ì› ë‚´ìš© ë¶„ì„
        - ì‹ ì²­ ë°©ë²• íŒŒì•…
        - ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
        """
        
        # 1. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        categories = self._classify_policy_category(title, content)
        
        # 2. ì—°ë ¹ ì¡°ê±´ ì¶”ì¶œ
        target_age = self._extract_age_condition(content)
        
        # 3. ì§€ì—­ ì •ë³´ ì¶”ì¶œ
        target_region = self._extract_region_info(content, agency)
        
        # 4. ì§€ì› í˜œíƒ ì¶”ì¶œ
        benefits = self._extract_benefits(content)
        
        # 5. ì‹ ì²­ ë°©ë²• ì¶”ì¶œ
        apply_method = self._extract_apply_method(content)
        
        # 6. ì‹ ì²­ URL ì¶”ì¶œ
        apply_url = self._extract_apply_url(content, url)
        
        # 7. ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
        contact_info = self._extract_contact_info(content)
        
        # 8. ìœ íš¨ê¸°ê°„ ì¶”ì¶œ
        valid_from, valid_to = self._extract_validity_period(content)
        
        # 9. ìš”ì•½ ìƒì„± (ì²« 200ì ë˜ëŠ” ì²« ë¬¸ì¥)
        summary = self._generate_summary(content)
        
        return PolicyData(
            title=title,
            summary=summary,
            content=content,
            agency=agency,
            category=categories,
            target_age=target_age,
            target_region=target_region,
            benefits=benefits,
            apply_method=apply_method,
            apply_url=apply_url,
            contact_info=contact_info,
            valid_from=valid_from,
            valid_to=valid_to,
            source_url=url,
            source_name=source
        )

    def _classify_policy_category(self, title: str, content: str) -> List[str]:
        """ì •ì±… ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
        text = (title + ' ' + content).lower()
        categories = []
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ìœ ì”¨ ì²­ë…„ì •ì±… 10ê°œ ì¹´í…Œê³ ë¦¬)
        category_keywords = {
            'ì·¨ì—…': [
                'ì·¨ì—…', 'ì¼ìë¦¬', 'êµ¬ì§', 'ì±„ìš©', 'ë©´ì ‘', 'ì´ë ¥ì„œ', 'ì¸í„´', 'ì§ë¬´',
                'ì·¨ì—…ì§€ì›', 'ì§ì—…í›ˆë ¨', 'ì·¨ì—…ì„±ê³µíŒ¨í‚¤ì§€', 'ì²­ë…„ì¸í„´', 'ì§ì—…ì²´í—˜', 'ê³ ìš©'
            ],
            'ì°½ì—…': [
                'ì°½ì—…', 'ìŠ¤íƒ€íŠ¸ì—…', 'ì‚¬ì—…', 'ê¸°ì—…', 'ì°½ì—…ì§€ì›', 'ì°½ì—…êµìœ¡', 'ì°½ì—…ìê¸ˆ',
                'ì‚¬ì—…ê³„íš', 'ì°½ì—…ë³´ìœ¡', 'ì°½ì—…ì¸íë² ì´íŒ…', 'ì˜ˆë¹„ì°½ì—…ê°€', 'ë²¤ì²˜', 'íˆ¬ì'
            ],
            'ì§„ë¡œ': [
                'ì§„ë¡œ', 'ê²½ë ¥', 'ì§ì—…ìƒë‹´', 'ì§„ë¡œìƒë‹´', 'ì»¤ë¦¬ì–´', 'ì§ì—…íƒìƒ‰', 'ì§„ë¡œì„¤ê³„',
                'ì§„ë¡œê°œë°œ', 'ì§ì—…ì²´í—˜', 'ë©˜í† ë§', 'ì§„ë¡œêµìœ¡'
            ],
            'ì£¼ê±°': [
                'ì›”ì„¸', 'ì „ì„¸', 'ì£¼ê±°', 'ì„ëŒ€', 'ë³´ì¦ê¸ˆ', 'ì£¼íƒ', 'ì›ë£¸', 'ì…°ì–´í•˜ìš°ìŠ¤',
                'ì£¼ê±°ë¹„', 'ì„ëŒ€ë£Œ', 'ì£¼ê±°ê¸‰ì—¬', 'ì£¼ê±°ì•ˆì •', 'ì²­ë…„ì£¼íƒ', 'ê±°ì£¼'
            ],
            'ê¸ˆìœµ': [
                'ëŒ€ì¶œ', 'ê¸ˆìœµ', 'ìê¸ˆ', 'ì§€ì›ê¸ˆ', 'ë³´ì¡°ê¸ˆ', 'ì¥í•™ê¸ˆ', 'ìƒí™œë¹„',
                'ê¸ˆìœµì§€ì›', 'ì†Œì•¡ëŒ€ì¶œ', 'ì²­ë…„ìˆ˜ë‹¹', 'ìƒí™œì•ˆì •ìê¸ˆ', 'ì ê¸ˆ', 'íˆ¬ì'
            ],
            'êµìœ¡': [
                'êµìœ¡', 'í•™ìŠµ', 'ê°•ì˜', 'ì—°ìˆ˜', 'êµìœ¡ë¹„', 'í•™ë¹„', 'êµìœ¡í”„ë¡œê·¸ë¨',
                'ì§ì—…êµìœ¡', 'í‰ìƒêµìœ¡', 'ì˜¨ë¼ì¸êµìœ¡', 'êµìœ¡ì§€ì›', 'ìˆ˜ê°•', 'ì—­ëŸ‰ê°œë°œ'
            ],
            'ì •ì‹ ê±´ê°•': [
                'ì •ì‹ ê±´ê°•', 'ì‹¬ë¦¬', 'ìƒë‹´', 'ìš°ìš¸', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ì •ì‹ ', 'ë§ˆìŒ', 'ì¹˜ë£Œ',
                'ì‹¬ë¦¬ìƒë‹´', 'ì •ì‹ ê³¼', 'ë§ˆìŒê±´ê°•', 'ì‹¬ë¦¬ì¹˜ë£Œ', 'ì •ì‹ ë³´ê±´'
            ],
            'ì‹ ì²´ê±´ê°•': [
                'ì‹ ì²´ê±´ê°•', 'ê±´ê°•', 'ì˜ë£Œ', 'ê²€ì§„', 'ìš´ë™', 'ì²´ë ¥', 'í—¬ìŠ¤', 'ê±´ê°•ê´€ë¦¬',
                'ì²´ìœ¡', 'ìŠ¤í¬ì¸ ', 'í”¼íŠ¸ë‹ˆìŠ¤', 'ê±´ê°•ê²€ì§„', 'ì˜ë£Œë¹„'
            ],
            'ìƒí™œì§€ì›': [
                'ìƒí™œì§€ì›', 'ë³µì§€', 'ìƒí™œë¹„', 'ê¸°ì´ˆ', 'ëŒë´„', 'ì¼ìƒ', 'í¸ì˜', 'ì§€ì›',
                'ìƒí™œì•ˆì •', 'ê¸°ì´ˆìƒí™œ', 'ë³µì§€ì„œë¹„ìŠ¤', 'ì‚¬íšŒë³µì§€', 'ìƒí™œë„ì›€'
            ],
            'ë¬¸í™”/ì˜ˆìˆ ': [
                'ë¬¸í™”', 'ì˜ˆìˆ ', 'ê³µì—°', 'ì „ì‹œ', 'ì²´í—˜', 'ë¬¸í™”í”„ë¡œê·¸ë¨', 'ë¬¸í™”ì˜ˆìˆ ',
                'ë¬¸í™”í™œë™', 'ì˜ˆìˆ êµìœ¡', 'ë¬¸í™”ì²´í—˜', 'ê³µì—°ê´€ëŒ', 'ì¶•ì œ', 'ì˜ˆìˆ í™œë™'
            ]
        }
        
        for category, keywords in category_keywords.items():
            if any(keyword in text for keyword in keywords):
                categories.append(category)
        
        return categories if categories else ['ê¸°íƒ€']

    def _extract_age_condition(self, content: str) -> Optional[Dict[str, int]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì—°ë ¹ ì¡°ê±´ ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ ì—°ë ¹ í‘œí˜„ íŒ¨í„´
        age_patterns = [
            r'ë§Œ?\s*(\d+)ì„¸\s*ì´ìƒ\s*(\d+)ì„¸\s*ì´í•˜',
            r'ë§Œ?\s*(\d+)ì„¸\s*~\s*(\d+)ì„¸',
            r'ë§Œ?\s*(\d+)ì„¸\s*ë¶€í„°\s*(\d+)ì„¸\s*ê¹Œì§€',
            r'(\d+)ì„¸\s*ì´ìƒ\s*(\d+)ì„¸\s*ë¯¸ë§Œ',
            r'ë§Œ?\s*(\d+)ì„¸\s*ì´ìƒ',
            r'(\d+)ì„¸\s*ë¯¸ë§Œ'
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, content)
            if match:
                groups = match.groups()
                if len(groups) == 2:
                    return {'min': int(groups[0]), 'max': int(groups[1])}
                elif len(groups) == 1:
                    if 'ì´ìƒ' in match.group(0):
                        return {'min': int(groups[0])}
                    elif 'ë¯¸ë§Œ' in match.group(0):
                        return {'max': int(groups[0]) - 1}
        
        return None

    def _extract_region_info(self, content: str, agency: str) -> str:
        """ì§€ì—­ ì •ë³´ ì¶”ì¶œ"""
        # ì„œìš¸ì‹œ 25ê°œ ìì¹˜êµ¬ ëª©ë¡
        districts = [
            'ê°•ë‚¨êµ¬', 'ê°•ë™êµ¬', 'ê°•ë¶êµ¬', 'ê°•ì„œêµ¬', 'ê´€ì•…êµ¬', 'ê´‘ì§„êµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬',
            'ë…¸ì›êµ¬', 'ë„ë´‰êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ë™ì‘êµ¬', 'ë§ˆí¬êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ì„œì´ˆêµ¬', 'ì„±ë™êµ¬',
            'ì„±ë¶êµ¬', 'ì†¡íŒŒêµ¬', 'ì–‘ì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ìš©ì‚°êµ¬', 'ì€í‰êµ¬', 'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ì¤‘ë‘êµ¬'
        ]
        
        # ê¸°ê´€ëª…ì—ì„œ êµ¬ ì •ë³´ ì¶”ì¶œ
        for district in districts:
            if district in agency:
                return district
        
        # ë‚´ìš©ì—ì„œ êµ¬ ì •ë³´ ì¶”ì¶œ
        for district in districts:
            if district in content:
                return district
        
        return 'ì„œìš¸ì‹œ ì „ì²´'

    def _extract_benefits(self, content: str) -> str:
        """ì§€ì› í˜œíƒ ì •ë³´ ì¶”ì¶œ"""
        # í˜œíƒ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ ì¶”ì¶œ
        benefit_keywords = ['ì§€ì›', 'í˜œíƒ', 'ì œê³µ', 'ê¸‰ì—¬', 'ìˆ˜ë‹¹', 'ëŒ€ì¶œ', 'ë³´ì¡°']
        
        sentences = content.split('.')
        benefit_sentences = []
        
        for sentence in sentences:
            if any(keyword in sentence for keyword in benefit_keywords):
                # ê¸ˆì•¡ ì •ë³´ê°€ í¬í•¨ëœ ë¬¸ì¥ ìš°ì„ 
                if re.search(r'\d+ë§Œì›|\d+ì›|\d+%', sentence):
                    benefit_sentences.insert(0, sentence.strip())
                else:
                    benefit_sentences.append(sentence.strip())
        
        return '. '.join(benefit_sentences[:3]) if benefit_sentences else content[:200]

    def _extract_apply_method(self, content: str) -> str:
        """ì‹ ì²­ ë°©ë²• ì¶”ì¶œ"""
        method_keywords = ['ì‹ ì²­', 'ì ‘ìˆ˜', 'ë°©ë²•', 'ì ˆì°¨', 'ì œì¶œ']
        
        sentences = content.split('.')
        method_sentences = []
        
        for sentence in sentences:
            if any(keyword in sentence for keyword in method_keywords):
                method_sentences.append(sentence.strip())
        
        return '. '.join(method_sentences[:2]) if method_sentences else 'ì˜¨ë¼ì¸ ë˜ëŠ” ë°©ë¬¸ ì‹ ì²­'

    def _extract_apply_url(self, content: str, base_url: str) -> str:
        """ì‹ ì²­ URL ì¶”ì¶œ"""
        # URL íŒ¨í„´ ë§¤ì¹­
        url_patterns = [
            r'https?://[^\s<>"]+',
            r'www\.[^\s<>"]+',
        ]
        
        for pattern in url_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if any(keyword in match.lower() for keyword in ['apply', 'sinchung', 'ì‹ ì²­', 'youth']):
                    return match
        
        return base_url  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì •ì±… í˜ì´ì§€ URL ë°˜í™˜

    def _extract_contact_info(self, content: str) -> str:
        """ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        contact_patterns = [
            r'(\d{2,3}-\d{3,4}-\d{4})',  # ì „í™”ë²ˆí˜¸
            r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',  # ì´ë©”ì¼
        ]
        
        contacts = []
        for pattern in contact_patterns:
            matches = re.findall(pattern, content)
            contacts.extend(matches)
        
        # ë‹´ë‹¹ ë¶€ì„œ ì¶”ì¶œ
        dept_keywords = ['ê³¼', 'íŒ€', 'ì„¼í„°', 'ë¶€ì„œ']
        sentences = content.split('.')
        
        for sentence in sentences:
            if any(keyword in sentence for keyword in dept_keywords) and 'ë¬¸ì˜' in sentence:
                contacts.append(sentence.strip())
                break
        
        return ', '.join(contacts) if contacts else 'í•´ë‹¹ ê¸°ê´€ ë¬¸ì˜'

    def _extract_validity_period(self, content: str) -> tuple:
        """ìœ íš¨ê¸°ê°„ ì¶”ì¶œ"""
        # ë‚ ì§œ íŒ¨í„´ ë§¤ì¹­
        date_patterns = [
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
        ]
        
        dates = []
        for pattern in date_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                try:
                    year, month, day = map(int, match)
                    date_obj = datetime(year, month, day)
                    dates.append(date_obj)
                except:
                    continue
        
        if len(dates) >= 2:
            dates.sort()
            return dates[0], dates[-1]
        elif len(dates) == 1:
            # í•˜ë‚˜ì˜ ë‚ ì§œë§Œ ìˆìœ¼ë©´ ë§ˆê°ì¼ë¡œ ê°€ì •
            return datetime(2025, 1, 1), dates[0]
        else:
            # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì—°ë„ ê¸°ì¤€
            return datetime(2025, 1, 1), datetime(2025, 12, 31)

    def _generate_summary(self, content: str) -> str:
        """ì •ì±… ìš”ì•½ ìƒì„±"""
        # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ë‚˜ 200ì ì´ë‚´ë¡œ ìš”ì•½
        sentences = content.split('.')
        first_sentence = sentences[0].strip() if sentences else ''
        
        if len(first_sentence) > 200:
            return first_sentence[:200] + '...'
        elif len(first_sentence) < 50 and len(sentences) > 1:
            return (first_sentence + '. ' + sentences[1].strip())[:200] + '...'
        else:
            return first_sentence

    async def _parse_api_response(self, api_data: Dict, source_name: str) -> Optional[PolicyData]:
        """API ì‘ë‹µ ë°ì´í„°ë¥¼ PolicyDataë¡œ ë³€í™˜"""
        try:
            # API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ í•„ë“œ ë§¤í•‘
            title = api_data.get('POLICY_NM') or api_data.get('TITLE') or api_data.get('title', '')
            content = api_data.get('POLICY_CONTENT') or api_data.get('CONTENT') or api_data.get('content', '')
            
            if not title:
                return None
                
            # PolicyData ê°ì²´ ìƒì„±
            return self._analyze_policy_content(
                title=title,
                content=content,
                url=api_data.get('POLICY_URL', ''),
                source=source_name,
                agency=api_data.get('AGENCY_NM', 'ì„œìš¸íŠ¹ë³„ì‹œ')
            )
            
        except Exception as e:
            logger.error(f"âŒ API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    async def _crawl_seoul_main_site(self) -> List[PolicyData]:
        """ì„œìš¸ì‹œ ê³µì‹ í™ˆí˜ì´ì§€ í¬ë¡¤ë§"""
        logger.info("ğŸ›ï¸ ì„œìš¸ì‹œ ê³µì‹ í™ˆí˜ì´ì§€ í¬ë¡¤ë§...")
        
        policies = []
        site_config = self.target_sites['seoul_main_site']
        
        # êµ¬í˜„ ë¡œì§ì€ _crawl_youth_portalê³¼ ìœ ì‚¬
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê° ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ì— ë§ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§•
        
        return policies

    async def _crawl_district_sites(self) -> List[PolicyData]:
        """ìì¹˜êµ¬ ì‚¬ì´íŠ¸ë“¤ í¬ë¡¤ë§"""
        logger.info("ğŸ¢ ìì¹˜êµ¬ ì‚¬ì´íŠ¸ í¬ë¡¤ë§...")
        
        policies = []
        district_sites = self.target_sites['district_sites']
        
        for district, url in district_sites.items():
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        html = await response.text()
                        # ê° êµ¬ë³„ë¡œ ë‹¤ë¥¸ í˜ì´ì§€ êµ¬ì¡°ë¥¼ ê°€ì§€ë¯€ë¡œ 
                        # ë²”ìš©ì ì¸ íŒŒì‹± ë¡œì§ ì ìš©
                        district_policies = await self._parse_district_page(html, url, district)
                        policies.extend(district_policies)
                        
                await asyncio.sleep(1)  # ìì¹˜êµ¬ ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€
                
            except Exception as e:
                logger.warning(f"âš ï¸ {district} ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        return policies

    async def _parse_district_page(self, html: str, url: str, district: str) -> List[PolicyData]:
        """ìì¹˜êµ¬ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html, 'html.parser')
        policies = []
        
        # ë²”ìš©ì ì¸ ì •ì±… ì •ë³´ ì¶”ì¶œ ì‹œë„
        # ì œëª©, ë§í¬, ë‚´ìš©ì´ í¬í•¨ëœ ìš”ì†Œë“¤ì„ ì°¾ìŒ
        potential_policies = soup.find_all(['article', 'div', 'li'], 
                                         class_=re.compile(r'policy|notice|program|support'))
        
        for elem in potential_policies[:10]:  # MVP: êµ¬ë³„ ìµœëŒ€ 10ê°œ
            title_elem = elem.find(['h1', 'h2', 'h3', 'a'])
            if title_elem:
                title = title_elem.get_text().strip()
                if len(title) > 10 and any(keyword in title for keyword in ['ì²­ë…„', 'ì§€ì›', 'ì •ì±…']):
                    # ê°„ë‹¨í•œ ì •ì±… ë°ì´í„° ìƒì„±
                    policies.append(PolicyData(
                        title=title,
                        summary=title,
                        content=elem.get_text().strip()[:500],
                        agency=f"{district}êµ¬ì²­",
                        category=['ê¸°íƒ€'],
                        target_age=None,
                        target_region=district + 'êµ¬',
                        benefits='êµ¬ì²­ ë¬¸ì˜',
                        apply_method='êµ¬ì²­ ë°©ë¬¸ ë˜ëŠ” ì˜¨ë¼ì¸',
                        apply_url=url,
                        contact_info=f"{district}êµ¬ì²­",
                        valid_from=datetime(2025, 1, 1),
                        valid_to=datetime(2025, 12, 31),
                        source_url=url,
                        source_name=f"{district}êµ¬ì²­ í™ˆí˜ì´ì§€"
                    ))
        
        return policies

    async def _crawl_rss_feeds(self) -> List[PolicyData]:
        """RSS í”¼ë“œì—ì„œ ìµœì‹  ê³µê³  ìˆ˜ì§‘"""
        logger.info("ğŸ“¡ RSS í”¼ë“œ ìˆ˜ì§‘...")
        
        policies = []
        
        # ì„œìš¸ì‹œ RSS í”¼ë“œ ëª©ë¡
        rss_feeds = [
            'https://www.seoul.go.kr/news/rss.do?type=notice',
            'https://youth.seoul.go.kr/rss/policy.xml'  # ê°€ìƒì˜ ì²­ë…„ì •ì±… RSS
        ]
        
        for feed_url in rss_feeds:
            try:
                async with self.session.get(feed_url) as response:
                    if response.status == 200:
                        rss_content = await response.text()
                        feed = feedparser.parse(rss_content)
                        
                        for entry in feed.entries[:20]:  # ìµœì‹  20ê°œ
                            if any(keyword in entry.title.lower() for keyword in ['ì²­ë…„', 'ì •ì±…', 'ì§€ì›']):
                                policy = PolicyData(
                                    title=entry.title,
                                    summary=entry.get('summary', entry.title),
                                    content=entry.get('description', ''),
                                    agency='ì„œìš¸íŠ¹ë³„ì‹œ',
                                    category=self._classify_policy_category(entry.title, entry.get('description', '')),
                                    target_age=None,
                                    target_region='ì„œìš¸ì‹œ ì „ì²´',
                                    benefits='ê³µê³  ë‚´ìš© ì°¸ì¡°',
                                    apply_method='ê³µê³  ë‚´ìš© ì°¸ì¡°',
                                    apply_url=entry.link,
                                    contact_info='ì„œìš¸ì‹œì²­',
                                    valid_from=datetime.now(),
                                    valid_to=datetime(2025, 12, 31),
                                    source_url=entry.link,
                                    source_name='ì„œìš¸ì‹œ RSS'
                                )
                                policies.append(policy)
                                
            except Exception as e:
                logger.warning(f"âš ï¸ RSS í”¼ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ {feed_url}: {e}")
        
        return policies

    def _log_crawl_stats(self):
        """í¬ë¡¤ë§ í†µê³„ ë¡œê¹…"""
        logger.info("ğŸ“Š í¬ë¡¤ë§ í†µê³„:")
        for key, value in self.crawl_stats.items():
            logger.info(f"   â€¢ {key}: {value}")

    async def detect_changes(self, existing_policies: List[Dict]) -> List[str]:
        """
        ë³€ê²½ì‚¬í•­ ê°ì§€
        
        ê¸°ì¡´ ì •ì±…ê³¼ ìƒˆë¡œ í¬ë¡¤ë§í•œ ì •ì±…ì„ ë¹„êµí•˜ì—¬
        ë³€ê²½ëœ ì •ì±…ë“¤ì˜ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        changed_policies = []
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í•´ì‹œ ë¹„êµë‚˜ Last-Modified í—¤ë” í™•ì¸
        # MVPì—ì„œëŠ” ê°„ë‹¨í•œ URL ê¸°ë°˜ ë³€ê²½ ê°ì§€
        
        return changed_policies

# ========================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ========================================

async def run_real_time_crawling(api_key: str) -> List[PolicyData]:
    """
    ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹¤í–‰
    
    Args:
        api_key: ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API í‚¤
        
    Returns:
        List[PolicyData]: ìˆ˜ì§‘ëœ ì •ì±… ë°ì´í„°
    """
    async with SeoulYouthPolicyCrawler(api_key) as crawler:
        return await crawler.crawl_all_sources()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def test_crawler():
        api_key = "75786159696b6a6839324d7a674776"
        policies = await run_real_time_crawling(api_key)
        
        print(f"âœ… ì´ {len(policies)}ê°œ ì •ì±… ìˆ˜ì§‘ ì™„ë£Œ")
        for policy in policies[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            print(f"   â€¢ {policy.title} ({policy.agency})")
    
    asyncio.run(test_crawler())
